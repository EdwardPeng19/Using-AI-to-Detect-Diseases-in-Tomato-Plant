{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the required Keras dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "import keras\n",
    "from keras import regularizers, optimizers\n",
    "from keras.layers import Conv2D,Input,Dense,MaxPooling2D,BatchNormalization,ZeroPadding2D,Flatten,Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger,EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from numpy.random import permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet_Model():\n",
    "    # creating a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # adding first set of CONV -> RELU -> POOL\n",
    "    model.add(Convolution2D(20, 5, 5, border_mode=\"same\",input_shape=(60, 60,3))) # 3,60,60 tha pehle\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    # adding second set of CONV -> RELU -> POOL\n",
    "    model.add(Convolution2D(50, 5, 5, border_mode=\"same\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2),strides=(2, 2),dim_ordering='th')) # specifying the order of dimensions\n",
    "\n",
    "    # adding set of FC -> RELU layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation(\"relu\"))\n",
    " \n",
    "    # adding softmax classifier\n",
    "    model.add(Dense(7))          # 7 categories\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubham saksham\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(20, (5, 5), input_shape=(60, 60, 3..., padding=\"same\")`\n",
      "  \n",
      "C:\\Users\\shubham saksham\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(50, (5, 5), padding=\"same\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\shubham saksham\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(2, 2), strides=(2, 2), data_format=\"channels_first\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model= LeNet_Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a summary of various layers and total parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 60, 60, 20)        1520      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 60, 20)        0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 60, 60, 20)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 50)        25050     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 30, 50)        0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 30, 50)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 15, 25)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 11250)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               5625500   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 3507      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 5,655,577\n",
      "Trainable params: 5,655,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the train images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = np.load('train_images_lenet.npy')\n",
    "train_labels = np.load('train_labels_lenet.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6359, 60, 60, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape # 6359 images of 60x60 resolution and 3 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6359,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 591,  592,  590,   24,  592,  372, 3598], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_labels)\n",
    "np.bincount(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduce learning rate when a metric has stopped improving.\n",
    "lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)\n",
    "\n",
    "# Callback that streams epoch results to a csv file\n",
    "csv_logger = CSVLogger('Lenet.csv')\n",
    "\n",
    "# Stop training when a monitored quantity has stopped improving.\n",
    "early_stopper = EarlyStopping(min_delta=0.001,patience=30)\n",
    "\n",
    "# Save the model after every epoch.\n",
    "model_checkpoint = ModelCheckpoint('Lenet.hdf5',monitor = 'val_loss', verbose = 1,save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pretrained model\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "mean = np.mean(train_images,axis=(0,1,2,3))\n",
    "std = np.std(train_images,axis=(0,1,2,3))\n",
    "train_images = (train_images-mean)/(std+1e-7)\n",
    "num_classes = 7\n",
    "train_labels = np_utils.to_categorical(train_labels,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6359, 60, 60, 3), (6359, 7))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomly distributing train_images to make train_images and val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perm = permutation(len(train_images))\n",
    "train_images = train_images[perm]\n",
    "train_labels = train_labels[perm]\n",
    "new_train = train_images[1:4800]\n",
    "new_labels = train_labels[1:4800]\n",
    "val_images= train_images[4800:]\n",
    "val_labels = train_labels[4800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4799, 60, 60, 3), (1559, 60, 60, 3))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train.shape,val_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "        optimizer=\"Adam\",\n",
    "        metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "399/399 [==============================] - 123s 307ms/step - loss: 0.8658 - acc: 0.6816 - val_loss: 0.3305 - val_acc: 0.8942\n",
      "Epoch 2/30\n",
      "399/399 [==============================] - 121s 304ms/step - loss: 0.5086 - acc: 0.7744 - val_loss: 0.1888 - val_acc: 0.9416\n",
      "Epoch 3/30\n",
      "399/399 [==============================] - 122s 306ms/step - loss: 0.3764 - acc: 0.8183 - val_loss: 0.1186 - val_acc: 0.9660\n",
      "Epoch 4/30\n",
      "399/399 [==============================] - 121s 303ms/step - loss: 0.3479 - acc: 0.8279 - val_loss: 0.1644 - val_acc: 0.9416\n",
      "Epoch 5/30\n",
      "399/399 [==============================] - 121s 303ms/step - loss: 0.3341 - acc: 0.8294 - val_loss: 0.1734 - val_acc: 0.9480\n",
      "Epoch 6/30\n",
      "399/399 [==============================] - 120s 302ms/step - loss: 0.2879 - acc: 0.8525 - val_loss: 0.0855 - val_acc: 0.9692\n",
      "Epoch 7/30\n",
      "399/399 [==============================] - 122s 307ms/step - loss: 0.3068 - acc: 0.8423 - val_loss: 0.1244 - val_acc: 0.9609\n",
      "Epoch 8/30\n",
      "399/399 [==============================] - 121s 304ms/step - loss: 0.2708 - acc: 0.8517 - val_loss: 0.0982 - val_acc: 0.9647\n",
      "Epoch 9/30\n",
      "399/399 [==============================] - 122s 305ms/step - loss: 0.2504 - acc: 0.8619 - val_loss: 0.0674 - val_acc: 0.9769\n",
      "Epoch 10/30\n",
      "399/399 [==============================] - 125s 314ms/step - loss: 0.2727 - acc: 0.8475 - val_loss: 0.0755 - val_acc: 0.9782\n",
      "Epoch 11/30\n",
      "399/399 [==============================] - 121s 304ms/step - loss: 0.2716 - acc: 0.8495 - val_loss: 0.0882 - val_acc: 0.9750\n",
      "Epoch 12/30\n",
      "399/399 [==============================] - 121s 304ms/step - loss: 0.2266 - acc: 0.8636 - val_loss: 0.0512 - val_acc: 0.9852\n",
      "Epoch 13/30\n",
      "399/399 [==============================] - 122s 307ms/step - loss: 0.3050 - acc: 0.8448 - val_loss: 0.0539 - val_acc: 0.9827\n",
      "Epoch 14/30\n",
      "399/399 [==============================] - 121s 303ms/step - loss: 0.2288 - acc: 0.8584 - val_loss: 0.0789 - val_acc: 0.9775\n",
      "Epoch 15/30\n",
      "399/399 [==============================] - 121s 303ms/step - loss: 0.2459 - acc: 0.8515 - val_loss: 0.0836 - val_acc: 0.9782\n",
      "Epoch 16/30\n",
      "399/399 [==============================] - 121s 304ms/step - loss: 0.2579 - acc: 0.8531 - val_loss: 0.0818 - val_acc: 0.9756\n",
      "Epoch 17/30\n",
      "399/399 [==============================] - 121s 304ms/step - loss: 0.2353 - acc: 0.8682 - val_loss: 0.0599 - val_acc: 0.9846\n",
      "Epoch 18/30\n",
      "399/399 [==============================] - 121s 304ms/step - loss: 0.2389 - acc: 0.8594 - val_loss: 0.0505 - val_acc: 0.9846\n",
      "Epoch 19/30\n",
      "399/399 [==============================] - 124s 311ms/step - loss: 0.2048 - acc: 0.8692 - val_loss: 0.0858 - val_acc: 0.9731\n",
      "Epoch 20/30\n",
      "399/399 [==============================] - 122s 305ms/step - loss: 0.2122 - acc: 0.8661 - val_loss: 0.0909 - val_acc: 0.9692\n",
      "Epoch 21/30\n",
      "399/399 [==============================] - 122s 306ms/step - loss: 0.2357 - acc: 0.8649 - val_loss: 0.0703 - val_acc: 0.9859\n",
      "Epoch 22/30\n",
      "399/399 [==============================] - 122s 305ms/step - loss: 0.2153 - acc: 0.8676 - val_loss: 0.0795 - val_acc: 0.9782\n",
      "Epoch 23/30\n",
      "399/399 [==============================] - 122s 306ms/step - loss: 0.2263 - acc: 0.8626 - val_loss: 0.0544 - val_acc: 0.9865\n",
      "Epoch 24/30\n",
      "399/399 [==============================] - 122s 305ms/step - loss: 0.2605 - acc: 0.8588 - val_loss: 0.1958 - val_acc: 0.9500\n",
      "Epoch 25/30\n",
      "399/399 [==============================] - 121s 304ms/step - loss: 0.2525 - acc: 0.8628 - val_loss: 0.0398 - val_acc: 0.9885\n",
      "Epoch 26/30\n",
      "399/399 [==============================] - 122s 305ms/step - loss: 0.2376 - acc: 0.8680 - val_loss: 0.0868 - val_acc: 0.9737\n",
      "Epoch 27/30\n",
      "399/399 [==============================] - 121s 304ms/step - loss: 0.2196 - acc: 0.8638 - val_loss: 0.1115 - val_acc: 0.9654\n",
      "Epoch 28/30\n",
      "399/399 [==============================] - 122s 305ms/step - loss: 0.2231 - acc: 0.8669 - val_loss: 0.0716 - val_acc: 0.9846\n",
      "Epoch 29/30\n",
      "399/399 [==============================] - 122s 306ms/step - loss: 0.2345 - acc: 0.8642 - val_loss: 0.1660 - val_acc: 0.9609\n",
      "Epoch 30/30\n",
      "399/399 [==============================] - 129s 322ms/step - loss: 0.2105 - acc: 0.8694 - val_loss: 0.0473 - val_acc: 0.9891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e290a7bda0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen.fit(new_train)\n",
    "model.fit_generator(datagen.flow(new_train, new_labels, batch_size=12),\n",
    "                        steps_per_epoch=new_train.shape[0] // 12,\n",
    "                        epochs=30,\n",
    "                        verbose=1,\n",
    "                        validation_data=(val_images,val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('Tomato_diseases.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x1e2911da5f8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "loaded_model= load_model('Tomato_diseases.h5')\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = {0:'bacterial_spot',1:'Healthy',2:'late_blight',3:'leaf_mold',4:'septorial_leaf_spot',5:'mosaic_virus',\n",
    "               6:'yelow_curved'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model with random images of leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.37608969e-07   1.69465206e-11   4.41435226e-08   5.01497011e-10\n",
      "    9.96732354e-01   3.26405396e-03   2.98724240e-06]]\n",
      "4\n",
      "septorial_leaf_spot\n"
     ]
    }
   ],
   "source": [
    "input_img= cv2.imread('dataset\\septoriaLeafSpot\\sep3.jpg', 1)\n",
    "if input_img is not None:\n",
    "    img = cv2.resize(input_img, (60,60)).astype(np.float32)\n",
    "            \n",
    "else:\n",
    "    print(\"image not loaded\")\n",
    "    \n",
    "img = (img-mean)/(std+1e-7)\n",
    "img = np.expand_dims(img, axis=0)  \n",
    "out = loaded_model.predict(img) \n",
    "print(out)\n",
    "print(np.argmax(out))\n",
    "print(class_names[np.argmax(out)])\n",
    "# Correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.92730761e-06   5.03858688e-09   9.99797761e-01   1.55134074e-07\n",
      "    1.99062692e-04   2.72000374e-11   1.23168334e-07]]\n",
      "2\n",
      "late_blight\n"
     ]
    }
   ],
   "source": [
    "#input_img= cv2.imread('dataset\\lateblight\\l4.png', 1)\n",
    "\n",
    "input_img= cv2.imread(\"dataset/\"+'lateblight'+'/'+'l4.png', 1)\n",
    "\n",
    "if input_img is not None:\n",
    "    img = cv2.resize(input_img,(60,60)).astype(np.float32)\n",
    "else:\n",
    "    print(\"image not loaded\")\n",
    "    \n",
    "img = (img-mean)/(std+1e-7)\n",
    "img = np.expand_dims(img, axis=0)  \n",
    "out = loaded_model.predict(img) \n",
    "print(out)\n",
    "print(np.argmax(out))\n",
    "print(class_names[np.argmax(out)])\n",
    "# Correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
